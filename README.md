Hand gesture recognition is a very useful technological advancement as an alternative user interface for providing real-time data to a computer. Classical interaction tools like mouse, keyboard limit the way we interact with our system. Also,this can be very helpful for the deaf and dumb people in interacting and communicating with other people and also computer systems. It can also be used as a sign language translator for the deaf and dumb people. The aim of this project is thus to create a sign recognition detector using Deep Learning and CNN that can detect basic patterns like numbers or signs based on the trained data set being used. Existing gesture recognition software make use of a hardware system design with motion sensors that are not efficient. By implementing a CNN model using Tensor flow, this system aims to make use of just the device camera to live capture and detect the sign automatically. The data set can be freshly trained based on the user requirement. The future scope of the project will be to train the model to store and convert the text into program input or voice which may require more complex algorithms and understanding of Neural Networks and Deep Learning Technologies. Throughout the long term, correspondence has played an indispensable job in return of data and sentiments in one's day to day existence. Sign language is the main medium through which deaf and mute individuals can interact with rest of the world through various hand motions. With the advances in machine learning, it is possible to detect sign language in real time. We have utilized the OpenCV python library, Tensorflow Object Detection pipeline and transfer learning to train a deep learning model that detects sign languages in real time.
